\section{Desarrollo}

\subsection{Descomposici\'on SVD}

	\PARstart De la introducci\'on se deduce que no es necesario calcular de forma
	completa toda la descomposici\'on en valores singulares. Basta tener
	las primeras $k$ columnas de $V$. Es decir necesitamos un m\'etodo para
	obtener los autovectores que se corresponden con los autovalores de mayor
	m\'odulo de una matriz.
	Es importante notar que la matriz con la que trabajamos es sim\'etrica
	con coeficientes reales y por lo tanto podremos utilizar metodos optimizados.
	Para obtener los autovectores implementamos dos estrategias:

	\begin{itemize}
		\item \textbf{M\'etodo de la Potencia y deflaci\'on:} Suponiendo que
		tenemos autovalores $|\lambda_1| \geq \ldots \geq |\lambda_n|$ y los primeros
		$k$ son distintos dos a dos, obtenemos $\lambda_1$ (y su atovector asociado),
		mediante el m\'etodo de la potencia. Luego
		construimos una matriz que tenga como autovalores
		$|\lambda_2| \geq \ldots \geq |\lambda_n| \geq 0$ usando el m\'etodo
		de deflaci\'on.
		De esta forma inductiva calculamos los primeros $k$ autovalores
		con sus correspondientes autovectores.

		\item \textbf{M\'etodo QR para matrices sim\'etricas tridiagonales y
		m\'etodo de la potencia inversa:} Asumiendo que partimos de una matriz
		sim\'etrica, obtenemos una matriz de hessemberg (y en este caso
		tridiagonal, por ser sim\'etrica) semejante, mediante reflexiones de
		Householder. Luego, mediante el algoritmo QR calculamos sus
		autovalores. Finalemente usamos esta aproximaci\'on de
		los autovalores para calcular los primeros $k$ autovectores con el m\'etodo
		de la potencia inversa.
	\end{itemize}

	A continuaci\'on se explican las implementaciones con m\'as detalle.

	\subsubsection{M\'etodo de la potencia y deflaci\'on}
		El m\'etodo de la potencia es un m\'etodo iterativo. Se parte con un vector
		inicial $v_0$ y se genera una sucesi\'on de vectores ${v_i}$ realizando,
		en cada iteraci\'on, $$v_{i+1} = \frac{Av_i}{\|Av_i\|_{\infty}}$$
		Asumiendo que la matriz tiene un autovalor $\bar{\lambda}$ de m\'odulo
		estrictamente
		mayor al resto de los autovalores y que $v_0$ no es ortogonal al
		autovector $\bar{v}$ asociado a dicho autovalor, la sucesi\'on converge 
		a $\bar{v}$.

		El m\'etodo no depende fuertemente de la norma que se usa para renormalizar
		en cada iteraci\'on. La \'unica diferencia es que el autovector obtenido
		estar\'a normalizado con la norma usada. En nuestro caso usamos la norma
		infinito ya que se calcula de manera r\'apida computacionalmente.

		Una vez que tenemos el autovalor $\bar{\lambda}$ obtenemos una matriz
		$A'$ que conserva los mismos autovalores y autovectores exceptuando
		$\bar{\lambda}$ y $\bar{v}$, realizando:
		$$A' = A-\bar{\lambda}\bar{v}\bar{v}^t$$

	\subsubsection{M\'etodo QR para matrices sim\'etricas tridiagonales y m\'etodo
	de la potencia inversa}

		Como la matriz de covarianza es sim\'etrica, utilizamos el algoritmo
		$9.5$ de \cite{burden} para obtener una matriz tridiagonal semejante a \'esta.
		El m\'etodo se basa en transformaciones de Householder.
		Una vez obtenida usamos el algoritmo $9.6$ para calcular sus autovalores.
		Dado que trabajamos con matrices tridiagonales los autovalores pueden
		calcularse r\'apidamente y con mucha precisi\'on.

		Luego utilizamos el m\'etodo inverso de la potencia.
		Este es bastante similar al m\'etodo de la potencia pero permite calcular
		cualquier par autovalor-autovector asumiendo las m\'ismas hip\'otesis
		que el m\'etodo de la potencia con el agregado de tener una aproximaci\'on
		razonable de los autovalores correspondientes a los autovectores que
		quieren calcularse.

		Partiendo de la matriz $A$, un autovector $v_0$ y una aproximaci\'on a un
		autovalor $\lambda$ obtenemos una sucesi\'on ${v_i}$ de autovectores
		que converge al autovector asociado al autovalor $\lambda$.
		Para esto planteamos la iteraci\'on:
		$$v_{i+1} = (A-\lambda Id)^{-1}v_i$$

		Notemos que este es el concepto matem\'atico. En nuestra implementaci\'on
		no invertimos la matriz, sino que resolvemos un sistema de ecuaciones
		en cada iteraci\'on. Para alivianar el tiempo de c\'omputo realizamos
		una descomposici\'on LU de la matriz del sistema que nos sirve para
		todas las iteraciones que se realicen con un mismo autovalor $\lambda$.
	
		Dado que se usan transformaciones ortogonales, el m\'etodo QR suele
		brindar resultados de mucha calidad.

\subsection{Reconociendo d\'igitos usando las componentes principales}

	\PARstart Para reconocer d\'igitos a partir de sus componentes principales
	implementamos tres m\'etodos. Cada uno de los metodos puede ser usado con la
	cantididad de componentes principales que se desee y por lo tanto no se
	har\'a referencia a dicha cantidad en la explicaci\'on. En la parte tests
	experimentaremos con diversas cantidades y buscaremos la m\'as adecuada para
	cada m\'etodo, ya que, veremos, que los mismos suelen requerir diversas
	cantidades para comportarse de forma \'optima.

	\subsubsection{kVecinos}
		Se calculan las componentes principales $c$ de la imagen a evaluar
		y las componentes $c_i$ de todas las im\'agenes $x_i$ de la base
		de datos. Se toman las distancias eucl\'ideas $\|c - c_i\|$
		y se cuentan las apariciones de cada d\'igito ($0, \ldots, 9$)
		en el conjunto de los $k$ $c_i$ m\'as cercanos a $c$.
		El d\'igito que registre mayor frecuencia se tomar\'a como el
		el d\'igito escrito en la imagen a evaluar.
	
	\subsubsection{kVecinos ponderados}
		En un principio pensamos calcular el promedio de las distancias $\|c - cj_i\|$.
		Donde $cj$ representa el conjunto de componentes principales de las
		im\'agenes del d\'igito $j$. De esta manera se est\'a tomando la 
		distancia promedio a todos los d\'igitos $0$, a los d\'igitos $1$, etc.

		Vimos que este m\'etodo no daba resultados razonables y parametrizamos
		el mismo, para dar m\'as granularidad a la hora de utilizarlo.
		En lugar de promediar la distancia a todos los d\'igitos de la base
		de datos, lo hacemos \'unicamente con los $k$ d\'igitos que se
		encuentren m\'as cerca.
		
	\subsubsection{Distancia al promedio de las componentes}
		Para cada d\'igito $0, \ldots, 9$ se iteran todos los $c_i$ 
		y se generan diez $\bar{c}_j$ correspondientes con el
		promedio componente a componente de los $c_i$ de los
		d\'igitos $j$.
		Luego se calcula la distancia de $c$ a cada uno de los $\bar{c}_j$
		y se toma como d\'igito escrito al $j$ tal que $\|c - \bar{c}_j\|$
		sea m\'inimo.

\subsection{Tests}

	\subsubsection{Error de los autovectores calculados}
		Para los dos m\'etodos utilizados para calcular autovectores
		dise\~namos un experimento que busca cuantificar el error m\'aximo
		que presentan los resultados.
		Se calculan los primeros $k$ autovectores $v_i$ y se normalizan con la
		norma eucl\'idea. Luego se realiza $Av_i$ para cada $i$ y se normaliza
		el resultado, obteniendo $\hat{v}_i$. Finalmente se toman las diferencias
		$\|v_i-\hat{v}_i\|$. Mientras m\'as precisos sean los resultados del
		algoritmo, m\'as peque\~nas ser\'an estas distancias.
		De esta manera podemos comparar el comportamiento de los dos algoritmos
		con distintas tolerancias de entrada.
		La definici\'on anterior est\'a sujeta a un problema:
		un autovector se encuentra contenido en una recta y esa recta contiene
		dos vectores de igual norma. Ambos vectores son autovectores y la \'unica
		diferencia entre ellos es el sentido. Teniendo esto en consideraci\'on
		podr\'ia ocurrir que la distancia calculada est\'e por arriba de $\sqrt{2}$
		e incluso que sea igual a $2$\footnote{En el caso en que obtengamos el mismo
		vector pero cambiado de signo.}. Por este motivo, cuando se obtiene una
		distancia mayor a $\sqrt{2}$ se multiplica a uno de los vectores por
		$-1$ y se la recalcula.
		
		Esperamos obtener resultados m\'as precisos al utilizar la estrategia
		\textit{QR-potencia Inversa}. Basamos esta hip\'otesis en que se usan
		matrices ortogonales para calcular los autovalores. Estas matrices
		tienen el n\'umero de condici\'on m\'as bajo posible, $1$.
		Una vez que tenemos aproximaciones de los autovalores de buena calidad
		el m\'etodo de la potencia inversa brinda buenas aproximaciones de los
		autovectores.

	\subsubsection{Diferencias en el tiempo de ejecuci\'on para obtener autovectores}
		Dado que ambos m\'etodos son esencialmente distintos cabe preguntarse
		si sus complejidades son distintas. Te\'oricamente notamos una diferencia
		importante: el m\'etodo de la potencia inversa resuelve un sistema de
		ecuaciones por cada autovector y por ende tiene una complejidad de $\Omega(n^3)$,
		donde $n$ representa la dimensi\'on de la matriz. Por otro lado el m\'etodo
		de la potencia s\'olo realiza una multiplicaci\'on entre matriz y vector
		y por lo tanto tiene una complejidad de $\Omega(n^2)$.
	
		Si bien estas son solamente cotas inferiores esperamos ver una
		diferencia significativa en el tiempo de ejecuci\'on de ambas
		estrategias.

	\subsubsection{Cantidad de d\'igitos reconocidos en funci\'on del m\'etodo,
	la cantidad de componentes utilizada y la precisi\'on de los autovectores}
		Comenzamos buscando buenos par\'ametros para los m\'etodos de reconocimiento
		de d\'igitos. Para esto calculamos autovectores con una buena precisi\'on
		y experimentamos con la cantidad de componentes entre $20$ y $100$.
		Una vez que obtenemos los par\'ametros que se comportan mejor para cada
		m\'etodo analizamos la diferencia de comporamiento de los m\'etodos
		al variar la cantidad de componentes principales y la precisi\'on con
		la cual se calculan los autovectores.

	\subsubsection{Influencia del tama\~no de la base de datos}
		Bas\'andonos en el test anterior comparamos el \textit{hit rate} logrado
		por los m\'etodos de reconocimiento de d\'igitos al variar al cantidad
		de im\'agenes en la base de datos.
		
	\subsubsection{D\'igitos mejor reconocidos en funci\'on del m\'etodo}
		Dado que implementamos m\'as de un m\'etodo para el reconocimiento de
		d\'igitos, nos preguntamos si los mismos funcionan mejor en distintos
		d\'igitos. Para responder al interrogante calculamos el \textit{hit rate}
		de cada d\'igito por separado y comparamos los resultados de los
		distintos m\'etodos.
